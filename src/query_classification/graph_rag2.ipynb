{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f76c369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 16:29:49,259 DEBUG:neo4j.pool: [#0000]  _: <POOL> created, direct address IPv4Address(('localhost', 7687))\n",
      "2025-05-10 16:30:02,240 INFO:root: User query: Find intermediate courses about AWS Lambda\n",
      "2025-05-10 16:30:02,293 DEBUG:httpcore.connection: close.started\n",
      "2025-05-10 16:30:02,295 DEBUG:httpcore.connection: close.complete\n",
      "2025-05-10 16:30:02,296 DEBUG:httpcore.connection: connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None\n",
      "2025-05-10 16:30:02,296 DEBUG:httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B52AAA8B30>\n",
      "2025-05-10 16:30:02,300 DEBUG:httpcore.http11: send_request_headers.started request=<Request [b'POST']>\n",
      "2025-05-10 16:30:02,300 DEBUG:httpcore.http11: send_request_headers.complete\n",
      "2025-05-10 16:30:02,306 DEBUG:httpcore.http11: send_request_body.started request=<Request [b'POST']>\n",
      "2025-05-10 16:30:02,307 DEBUG:httpcore.http11: send_request_body.complete\n",
      "2025-05-10 16:30:02,309 DEBUG:httpcore.http11: receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-05-10 16:33:00,271 DEBUG:httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sat, 10 May 2025 09:33:00 GMT'), (b'Transfer-Encoding', b'chunked')])\n",
      "2025-05-10 16:33:00,398 INFO:httpx: HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-05-10 16:33:00,404 DEBUG:httpcore.http11: receive_response_body.started request=<Request [b'POST']>\n",
      "2025-05-10 16:33:00,417 DEBUG:httpcore.http11: receive_response_body.complete\n",
      "2025-05-10 16:33:00,417 DEBUG:httpcore.http11: response_closed.started\n",
      "2025-05-10 16:33:00,417 DEBUG:httpcore.http11: response_closed.complete\n",
      "2025-05-10 16:33:00,467 DEBUG:root: === Raw Qwen response start ===\n",
      "2025-05-10 16:33:00,467 DEBUG:root: <think>\n",
      "        First locate the Skill node \"AWS Lambda\", then collect all Courses teaching that skill, then filter those by Level = \"Intermediate\". Remember to put those sub-process in list: [{...},..., {...}] for example.\n",
      "</think>\n",
      "\n",
      "\n",
      "2025-05-10 16:33:00,467 DEBUG:root: === Raw Qwen response end ===\n",
      "2025-05-10 16:33:00,486 DEBUG:root: JSON extraction attempt 2 failed: Expecting value: line 1 column 1 (char 0)\n",
      "2025-05-10 16:33:00,491 DEBUG:root: JSON extraction attempt 3 failed: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
      "2025-05-10 16:33:00,493 ERROR:root: Error extracting JSON: Failed to parse JSON from LLM response\n",
      "2025-05-10 16:33:00,494 ERROR:root: Raw response causing error: <think>\n",
      "        First locate the Skill node \"AWS Lambda\", then collect all Courses teaching that skill, then filter those by Level = \"Intermediate\". Remember to put those sub-process in list: [{...},..., {...}] for example.\n",
      "</think>\n",
      "\n",
      "\n",
      "2025-05-10 16:33:00,496 ERROR:root: Query processing failed: Failed to parse query steps from LLM response: Failed to parse JSON from LLM response\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20096\\2562684925.py\", line 146, in extract_json_from_text\n",
      "    return json.loads(json_str)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Python\\Lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Python\\Lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Python\\Lib\\json\\decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20096\\2562684925.py\", line 239, in analyze_query\n",
      "    steps = self.extract_json_from_text(raw)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20096\\2562684925.py\", line 149, in extract_json_from_text\n",
      "    raise ValueError(\"Failed to parse JSON from LLM response\")\n",
      "ValueError: Failed to parse JSON from LLM response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20096\\2562684925.py\", line 366, in process_query\n",
      "    steps = self.query_processor.analyze_query(user_query)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20096\\2562684925.py\", line 244, in analyze_query\n",
      "    raise RuntimeError(f\"Failed to parse query steps from LLM response: {e}\")\n",
      "RuntimeError: Failed to parse query steps from LLM response: Failed to parse JSON from LLM response\n",
      "Top matching courses:\n",
      "1. An error occurred while processing your query\n",
      "Exiting.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Neo4jConnection' object has no attribute 'driver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 539\u001b[0m\n\u001b[0;32m    537\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 539\u001b[0m     \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 509\u001b[0m, in \u001b[0;36mKnowledgeBaseQA.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneo4j_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m, in \u001b[0;36mNeo4jConnection.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Neo4jConnection' object has no attribute 'driver'"
     ]
    }
   ],
   "source": [
    "import logging, sys\n",
    "\n",
    "# Xoá toàn bộ handler hiện có\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "\n",
    "# Tạo handler mới ghi ra stdout\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s:%(name)s: %(message)s\"))\n",
    "\n",
    "# Gắn vào root logger, và bật mức DEBUG\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "root.addHandler(handler)\n",
    "\n",
    "import streamlit as st\n",
    "from neo4j import GraphDatabase\n",
    "import ollama\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "# Neo4j Connection\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(\n",
    "            uri, \n",
    "            auth=(user, password),\n",
    "            max_connection_pool_size=50,\n",
    "            connection_acquisition_timeout=30\n",
    "        )\n",
    "        \n",
    "    def run_query(self, query, parameters=None):\n",
    "        try:\n",
    "            with self._driver.session(\n",
    "                database=\"neo4j\",\n",
    "                default_access_mode=GraphDatabase.WRITE_ACCESS\n",
    "            ) as session:\n",
    "                result = session.run(query, parameters or {})\n",
    "                return [dict(r.items()) for r in result]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Neo4j query failed: {str(e)}\")\n",
    "            logging.debug(f\"Failed query: {query}\")\n",
    "            logging.debug(f\"Parameters: {parameters}\")\n",
    "            return []\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    # def run_query(self, query, parameters=None):\n",
    "    #     with self.driver.session() as session:\n",
    "    #         result = session.run(query, parameters or {})\n",
    "    #         records = [r.data() for r in result]\n",
    "\n",
    "\n",
    "    #     cleaned = []\n",
    "    #     for record in records:\n",
    "    #         clean = {}\n",
    "    #         for k, v in record.items():\n",
    "    #             new_key = k.strip().strip('\"').strip()\n",
    "    #             clean[new_key] = v\n",
    "    #         cleaned.append(clean)\n",
    "\n",
    "    #     return cleaned\n",
    "    # def run_query(self, query, parameters=None):\n",
    "    #     try:\n",
    "    #         with self.driver.session() as session:\n",
    "    #             return session.execute_write(\n",
    "    #                 lambda tx: list(tx.run(query, parameters or {})))\n",
    "    #     except Exception as e:\n",
    "    #         logging.error(f\"Query failed: {str(e)}\")\n",
    "    #         return []\n",
    "\n",
    "    def store_embedding(self, node_id, embedding):\n",
    "        query = \"\"\"\n",
    "        MATCH (c:Course {url: $node_id})\n",
    "        SET c.embedding = $embedding\n",
    "        \"\"\"\n",
    "        self.run_query(query, {\"node_id\": node_id, \"embedding\": embedding.tolist()})\n",
    "\n",
    "# LSTM Embedding Model\n",
    "class LSTMEmbeddingModel:\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=128, max_length=200):\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        self.max_length = max_length\n",
    "        self.model = Sequential([\n",
    "            Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "            LSTM(64, return_sequences=False),\n",
    "            Dense(embedding_dim, activation='tanh')\n",
    "        ])\n",
    "\n",
    "    def fit(self, texts):\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(sequences, maxlen=self.max_length, padding='post')\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "        dummy_labels = np.zeros((len(texts), 128))\n",
    "        self.model.fit(padded, dummy_labels, epochs=1, verbose=0)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        sequence = self.tokenizer.texts_to_sequences([text])\n",
    "        padded = pad_sequences(sequence, maxlen=self.max_length, padding='post')\n",
    "        return self.model.predict(padded, verbose=0)[0]\n",
    "\n",
    "# Query Processor with Qwen/deepseek\n",
    "class QueryProcessor:\n",
    "    def __init__(self, qwen_model):\n",
    "        self.qwen_model = qwen_model\n",
    "        # Improved regex pattern to better capture JSON array\n",
    "        self.json_pattern = re.compile(r'\\[\\s*{.*}\\s*\\]', re.DOTALL)\n",
    "\n",
    "    def extract_json_from_text(self, text):\n",
    "        \"\"\"Extract JSON array from text handling multiple formats\"\"\"\n",
    "        # Try to find JSON part with more flexible parsing\n",
    "        json_pattern = re.compile(\n",
    "            r'```json\\s*(\\[.*?\\])\\s*```', \n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Attempt 1: Look for ```json ``` blocks\n",
    "        match = json_pattern.search(text)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1))\n",
    "            except Exception as e:\n",
    "                logging.debug(f\"JSON extraction attempt 1 failed: {e}\")\n",
    "\n",
    "        # Attempt 2: Look after </think> tag\n",
    "        parts = text.split('</think>')\n",
    "        if len(parts) > 1:\n",
    "            json_part = parts[-1].strip()\n",
    "            try:\n",
    "                # Remove any non-JSON characters before/after\n",
    "                json_part = json_part[json_part.find('['):json_part.rfind(']')+1]\n",
    "                return json.loads(json_part)\n",
    "            except Exception as e:\n",
    "                logging.debug(f\"JSON extraction attempt 2 failed: {e}\")\n",
    "\n",
    "        # Attempt 3: Find first valid JSON array\n",
    "        try:\n",
    "            json_str = re.search(r'(\\[.*?\\])', text, re.DOTALL).group(1)\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"JSON extraction attempt 3 failed: {e}\")\n",
    "            raise ValueError(\"Failed to parse JSON from LLM response\")\n",
    "    def analyze_query(self, user_query):\n",
    "        prompt = \"\"\"\n",
    "        You are an expert in creating Cypher queries for Neo4j to answer complex, multihop questions about course data. The knowledge graph contains the following nodes and relationships:\n",
    "\n",
    "        - Nodes:\n",
    "        - Course: Represents a course in the dataset, with properties like url (unique identifier), name (course title), duration (length in hours), rating (user rating), description (course summary), embedding (vector representation).\n",
    "        - Skill: Represents a skill taught by a course, with properties like name (e.g., 'AWS Lambda', 'Python').\n",
    "        - Level: Represents the difficulty level of a course, with properties like name (e.g., 'Beginner', 'Intermediate', 'Advanced').\n",
    "        - Organization: Represents the organization offering the course, with properties like name (e.g., 'Coursera', 'Udemy').\n",
    "        - Instructor: Represents the instructor teaching the course, with properties like name (e.g., 'John Doe').\n",
    "        - Career: Represents a career path, with properties like name (e.g., 'Cloud Computing', 'Data Science').\n",
    "\n",
    "        - Relationships:\n",
    "        - TEACHES: Connects Course to Skill (Course -> Skill), indicating the course teaches the skill.\n",
    "        - HAS_LEVEL: Connects Course to Level (Course -> Level), indicating the course's difficulty level.\n",
    "        - OFFERED_BY: Connects Course to Organization (Course -> Organization), indicating the course is offered by the organization.\n",
    "        - TAUGHT_BY: Connects Course to Instructor (Course -> Instructor), indicating the course is taught by the instructor.\n",
    "        - REQUIRES: Connects Career to Skill (Career -> Skill), indicating the career requires the skill.\n",
    "\n",
    "        Your task is to analyze the natural language query below and **decompose** it into a **minimal**, **sequential** chain of Cypher sub-queries.  \n",
    "\n",
    "        1. First output a '<think> ... </think>' block with your step-by-step reasoning in plain English, describing:\n",
    "        - Which entity or relationship you'll target.\n",
    "        - Which filters you'll apply.\n",
    "        - How intermediate results will flow into the next step.\n",
    "\n",
    "        2. Then, immediately after '</think>', output **only** a JSON array. Each element must be an object with exactly these keys:\n",
    "        - \"description\": one-sentence summary of this step.\n",
    "        - \"cypher\": the full Cypher query.\n",
    "        - \"parameters\": a dict of named parameters (use $skill_name, $level, etc.) or refer to prior binds via \"$<bind_name>\".\n",
    "        - \"return\": either \"intermediate\" or \"final candidates\".\n",
    "        - \"bind\": (only for \"intermediate\" steps) the name you'll use to pass results forward (e.g. \"skill_node\", \"courses\").\n",
    "\n",
    "        3. **Do not** output any other text outside the <think> block and the JSON.\n",
    "\n",
    "        4. Escape all literal {{ and }} in the JSON by doubling them ({{/}}) if you plan to call Python's '.format()'.  \n",
    "\n",
    "        Example for query “Find beginner courses about AWS SageMaker”:\n",
    "\n",
    "        <think>\n",
    "        First locate the Skill node “AWS SageMaker”, then collect all Courses teaching that skill, then filter those by Level = “Beginner”. Remember to put those sub-process in list: [{{...}},..., {{...}}] for example.\n",
    "        </think>\n",
    "        '''json\n",
    "        [\n",
    "        {{\n",
    "            \"description\": \"Locate the AWS SageMaker skill node\",\n",
    "            \"cypher\": \"MATCH (sk:Skill {{name: $skill_name}}) RETURN sk AS skill_node\",\n",
    "            \"parameters\": {{\"skill_name\": \"AWS SageMaker\"}},\n",
    "            \"return\": \"intermediate\",\n",
    "            \"bind\": \"skill_node\"\n",
    "        }},\n",
    "        {{\n",
    "            \"description\": \"Find all courses that teach that skill\",\n",
    "            \"cypher\": \"MATCH (c:Course)-[:TEACHES]->(sk) RETURN collect(c) AS courses\",\n",
    "            \"parameters\": {{\"sk\": \"$skill_node\"}},\n",
    "            \"return\": \"intermediate\",\n",
    "            \"bind\": \"courses\"\n",
    "        }},\n",
    "        {{\n",
    "            \"description\": \"Filter those courses to Beginner level\",\n",
    "            \"cypher\": \"UNWIND $courses AS c MATCH (c)-[:HAS_LEVEL]->(l:Level {{name: $level}}) RETURN c.url AS course_url, c.name AS course_title\",\n",
    "            \"parameters\": {{\"courses\": \"$courses\", \"level\": \"Beginner\"}},\n",
    "            \"return\": \"final candidates\"\n",
    "        }}\n",
    "        ]\n",
    "        **Important Formatting Rules:**\n",
    "        - Wrap JSON output between ```json and ```\n",
    "        - Do NOT add any text/comments after JSON\n",
    "        - Use ONLY double quotes for JSON\n",
    "        - Escape special characters properly\n",
    "\n",
    "        Query: {user_query}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate response from LLM\n",
    "        resp = ollama.generate(\n",
    "            model=self.qwen_model,\n",
    "            prompt=prompt.format(user_query=user_query),\n",
    "            options={'stop': ['```']}  # Stop generation after JSON block\n",
    "        )\n",
    "\n",
    "        # Log the entire raw response\n",
    "        raw = resp[\"response\"]\n",
    "        logging.debug(\"=== Raw Qwen response start ===\")\n",
    "        logging.debug(raw)\n",
    "        logging.debug(\"=== Raw Qwen response end ===\")\n",
    "\n",
    "        # Extract JSON using improved method\n",
    "        try:\n",
    "            steps = self.extract_json_from_text(raw)\n",
    "            return steps\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting JSON: {e}\")\n",
    "            logging.error(f\"Raw response causing error: {raw}\")\n",
    "            raise RuntimeError(f\"Failed to parse query steps from LLM response: {e}\")\n",
    "\n",
    "# Knowledge Base QA System\n",
    "class KnowledgeBaseQA:\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password, qwen_model, embedding_model):\n",
    "        self.neo4j_conn = Neo4jConnection(neo4j_uri, neo4j_user, neo4j_password)\n",
    "        self.query_processor = QueryProcessor(qwen_model)\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def precompute_embeddings(self, courses):\n",
    "        descriptions = [course.get('description', '') for course in courses]\n",
    "        self.embedding_model.fit(descriptions)\n",
    "        for course in courses:\n",
    "            embedding = self.embedding_model.get_embedding(course.get('description', ''))\n",
    "            self.neo4j_conn.store_embedding(course.get('url', ''), embedding)\n",
    "\n",
    "    @lru_cache(maxsize=100)\n",
    "    # def process_query(self, user_query):\n",
    "    #     try:\n",
    "    #         # Analyze query with Qwen\n",
    "    #         steps = self.query_processor.analyze_query(user_query)\n",
    "            \n",
    "    #         if not steps or not isinstance(steps, list):\n",
    "    #             logging.error(f\"Invalid steps format returned: {steps}\")\n",
    "    #             return []\n",
    "                \n",
    "    #         intermediate_results = []\n",
    "    #         candidates = []\n",
    "\n",
    "    #         # Execute each subquery\n",
    "    #         for step_idx, step in enumerate(steps):\n",
    "    #             logging.debug(f\"Processing step {step_idx+1}: {step}\")\n",
    "                \n",
    "    #             # Validate step format\n",
    "    #             if not isinstance(step, dict) or \"cypher\" not in step:\n",
    "    #                 logging.error(f\"Invalid step format at index {step_idx}: {step}\")\n",
    "    #                 continue\n",
    "                    \n",
    "    #             cypher = step[\"cypher\"]\n",
    "    #             params = step.get(\"parameters\", {})\n",
    "    #             step_type = step.get(\"return\", \"\").lower()\n",
    "                \n",
    "    #             logging.debug(f\"Executing cypher: {cypher} with params: {params}\")\n",
    "                \n",
    "    #             if step_type == \"intermediate\":\n",
    "    #                 results = self.neo4j_conn.run_query(cypher, params)\n",
    "    #                 for record in results:\n",
    "    #                     # Extract various possible return values\n",
    "    #                     for key in [\"skill\", \"description\", \"name\"]:\n",
    "    #                         if key in record and record[key]:\n",
    "    #                             intermediate_results.append(record[key])\n",
    "    #                             break\n",
    "                                \n",
    "    #                 logging.debug(f\"Intermediate results: {intermediate_results}\")\n",
    "                    \n",
    "    #             elif step_type == \"candidates\":\n",
    "    #                 # If we have intermediate results, use them in the query\n",
    "    #                 if intermediate_results and params:\n",
    "    #                     # Find a parameter that might accept a list of values\n",
    "    #                     list_param = None\n",
    "    #                     for param_name, param_value in params.items():\n",
    "    #                         if isinstance(param_value, list):\n",
    "    #                             list_param = param_name\n",
    "    #                             break\n",
    "                        \n",
    "    #                     if list_param:\n",
    "    #                         # Use intermediate results in the list parameter\n",
    "    #                         params[list_param] = intermediate_results\n",
    "    #                         results = self.neo4j_conn.run_query(cypher, params)\n",
    "    #                         for record in results:\n",
    "    #                             if \"course_name\" in record and record[\"course_name\"]:\n",
    "    #                                 candidates.append(record[\"course_name\"])\n",
    "    #                     else:\n",
    "    #                         # Process each intermediate result individually\n",
    "    #                         for item in intermediate_results:\n",
    "    #                             for param_name in params:\n",
    "    #                                 params_copy = params.copy()\n",
    "    #                                 params_copy[param_name] = item\n",
    "    #                                 results = self.neo4j_conn.run_query(cypher, params_copy)\n",
    "    #                                 for record in results:\n",
    "    #                                     if \"course_name\" in record and record[\"course_name\"]:\n",
    "    #                                         candidates.append(record[\"course_name\"])\n",
    "    #                 else:\n",
    "    #                     # No intermediate results or params, just run the query directly\n",
    "    #                     results = self.neo4j_conn.run_query(cypher, params)\n",
    "    #                     for record in results:\n",
    "    #                         if \"course_name\" in record and record[\"course_name\"]:\n",
    "    #                             candidates.append(record[\"course_name\"])\n",
    "                                \n",
    "    #                 logging.debug(f\"Candidates: {candidates}\")\n",
    "\n",
    "    #         # Rank candidates using embeddings\n",
    "    #         if candidates:\n",
    "    #             query_embedding = self.embedding_model.get_embedding(user_query)\n",
    "    #             similarities = []\n",
    "    #             unique_candidates = list(set(candidates))\n",
    "    #             logging.debug(f\"Unique candidates to rank: {unique_candidates}\")\n",
    "                \n",
    "    #             for course_name in unique_candidates:\n",
    "    #                 description = self.get_course_description(course_name)\n",
    "    #                 if description:\n",
    "    #                     course_embedding = self.embedding_model.get_embedding(description)\n",
    "    #                     sim = np.dot(query_embedding, course_embedding) / (\n",
    "    #                         np.linalg.norm(query_embedding) * np.linalg.norm(course_embedding) + 1e-8  # Add small epsilon to avoid division by zero\n",
    "    #                     )\n",
    "    #                     similarities.append((course_name, sim))\n",
    "    #                 else:\n",
    "    #                     # If no description, use a default low similarity\n",
    "    #                     similarities.append((course_name, 0.0))\n",
    "\n",
    "    #             similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    #             return [x[0] for x in similarities[:5]]\n",
    "    #         return []\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         logging.error(f\"Error in process_query: {e}\", exc_info=True)\n",
    "    #         raise\n",
    "\n",
    "    # \n",
    "    \n",
    "    def process_query(self, user_query):\n",
    "        try:\n",
    "            steps = self.query_processor.analyze_query(user_query)\n",
    "            if not isinstance(steps, list):\n",
    "                return []\n",
    "\n",
    "            intermediate = {}\n",
    "            candidates = []\n",
    "            \n",
    "            for step_idx, step in enumerate(steps):\n",
    "                # Validate step structure\n",
    "                if not self._validate_step(step, step_idx):\n",
    "                    continue\n",
    "\n",
    "                # Process parameters with type checking\n",
    "                processed_params = self._process_parameters(step, intermediate)\n",
    "                \n",
    "                # Execute query with error handling\n",
    "                records = self._execute_cypher(\n",
    "                    step[\"cypher\"], \n",
    "                    processed_params, \n",
    "                    step_idx\n",
    "                )\n",
    "                if records is None:  # Execution failed\n",
    "                    return []\n",
    "\n",
    "                # Handle results\n",
    "                if step[\"return\"].lower() == \"intermediate\":\n",
    "                    self._handle_intermediate(step, records, intermediate)\n",
    "                else:\n",
    "                    self._collect_candidates(records, candidates)\n",
    "\n",
    "            return self._rank_results(candidates, user_query)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Query processing failed: {str(e)}\", exc_info=True)\n",
    "            return [\"An error occurred while processing your query\"]\n",
    "\n",
    "    # Helper methods\n",
    "    def _validate_step(self, step, step_idx):\n",
    "        required_keys = [\"description\", \"cypher\", \"return\"]\n",
    "        if not all(k in step for k in required_keys):\n",
    "            logging.error(f\"Missing keys in step {step_idx+1}: {step}\")\n",
    "            return False\n",
    "        if step[\"return\"].lower() == \"intermediate\" and \"bind\" not in step:\n",
    "            logging.error(f\"Missing bind key in intermediate step {step_idx+1}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _process_parameters(self, step, intermediate):\n",
    "        processed = {}\n",
    "        for k, v in step.get(\"parameters\", {}).items():\n",
    "            if isinstance(v, str) and v.startswith(\"$\"):\n",
    "                key = v[1:]\n",
    "                processed[k] = intermediate.get(key)\n",
    "                \n",
    "                # Add type conversion for Neo4j\n",
    "                if isinstance(processed[k], str) and 'node' in key:\n",
    "                    processed[k] = GraphDatabase.types.Node(\n",
    "                        element_id='dummy_id',\n",
    "                        labels=['Skill' if 'skill' in key else 'Course'],\n",
    "                        properties={'name': processed[k]}\n",
    "                    )\n",
    "            else:\n",
    "                processed[k] = v\n",
    "        return processed\n",
    "\n",
    "    def _execute_cypher(self, cypher, params, step_idx):\n",
    "        try:\n",
    "            return self.neo4j_conn.run_query(cypher, params)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Query failed at step {step_idx+1}: {e}\")\n",
    "            logging.debug(f\"Failed query: {cypher}\")\n",
    "            logging.debug(f\"Parameters: {params}\")\n",
    "            return None\n",
    "\n",
    "    def _handle_intermediate(self, step, records, intermediate):\n",
    "        bind_key = step[\"bind\"]\n",
    "        if records and bind_key in records[0]:\n",
    "            node = records[0][bind_key]\n",
    "            \n",
    "            # Handle different return types\n",
    "            if isinstance(node, GraphDatabase.types.Node):\n",
    "                if 'Skill' in node.labels:\n",
    "                    intermediate[bind_key] = node.get('name')\n",
    "                elif 'Course' in node.labels:\n",
    "                    intermediate[bind_key] = node.get('url')\n",
    "            elif isinstance(node, dict):\n",
    "                intermediate[bind_key] = node.get('name') or node.get('url')\n",
    "            else:\n",
    "                intermediate[bind_key] = node\n",
    "\n",
    "    def _collect_candidates(self, records, candidates):\n",
    "        for record in records:\n",
    "            if 'course_title' in record and 'course_url' in record:\n",
    "                candidates.append((\n",
    "                    record['course_title'],\n",
    "                    record['course_url']\n",
    "                ))\n",
    "\n",
    "    def _rank_results(self, candidates, query):\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        unique_courses = list(set(candidates))\n",
    "        \n",
    "        # Batch processing for efficiency\n",
    "        descriptions = {\n",
    "            title: self.get_course_description(title) \n",
    "            for title, _ in unique_courses\n",
    "        }\n",
    "        \n",
    "        query_embedding = self.embedding_model.get_embedding(query)\n",
    "        course_embeddings = {\n",
    "            title: self.embedding_model.get_embedding(desc) if desc \n",
    "            else np.zeros(self.embedding_model.model.output_shape[1])\n",
    "            for title, desc in descriptions.items()\n",
    "        }\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for title, url in unique_courses:\n",
    "            emb = course_embeddings[title]\n",
    "            norm = np.linalg.norm(query_embedding) * np.linalg.norm(emb) + 1e-8\n",
    "            sim = np.dot(query_embedding, emb) / norm\n",
    "            similarities.append((title, url, sim))\n",
    "        \n",
    "        # Return formatted results\n",
    "        return [\n",
    "            f\"{title} ({url})\" \n",
    "            for title, url, _ in sorted(\n",
    "                similarities, \n",
    "                key=lambda x: x[2], \n",
    "                reverse=True\n",
    "            )[:5]\n",
    "        ]\n",
    "\n",
    "    def get_course_description(self, course_name):\n",
    "        query = \"MATCH (c:Course {name: $name}) RETURN c.description AS description\"\n",
    "        result = self.neo4j_conn.run_query(query, {\"name\": course_name})\n",
    "        if result and \"description\" in result[0]:\n",
    "            return result[0][\"description\"]\n",
    "        return \"\"\n",
    "\n",
    "    def close(self):\n",
    "        self.neo4j_conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qa = KnowledgeBaseQA(\n",
    "        neo4j_uri=\"bolt://localhost:7687\",\n",
    "        neo4j_user=\"neo4j\",\n",
    "        neo4j_password=\"12345678\",\n",
    "        qwen_model=\"deepseek-r1:7b\",\n",
    "        embedding_model=LSTMEmbeddingModel()\n",
    "    )\n",
    "    try:\n",
    "        while True:\n",
    "            # Show prompt and read user input\n",
    "            user_query = input(\"Enter your query (e.g., Find intermediate courses about AWS Lambda): \")\n",
    "            if not user_query.strip():\n",
    "                print(\"Exiting.\")\n",
    "                break\n",
    "            logging.info(f\"User query: {user_query}\")\n",
    "            try:\n",
    "                results = qa.process_query(user_query)\n",
    "                if results:\n",
    "                    print(\"Top matching courses:\")\n",
    "                    for idx, course in enumerate(results, 1):\n",
    "                        print(f\"{idx}. {course}\")\n",
    "                else:\n",
    "                    print(\"No matching courses found.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing query: {e}\", exc_info=True)\n",
    "                print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        qa.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
